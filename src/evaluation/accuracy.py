"""Accuracy evaluation for multiple-choice questions."""

import re
from typing import List, Tuple


class AccuracyEvaluator:
    """Evaluator for multiple-choice question accuracy."""

    def __init__(self):
        """Initialize accuracy evaluator."""
        pass

    def extract_answer(self, text: str) -> str:
        """Extract answer choice (A, B, C, or D) from generated text.
        
        Looks for the first occurrence of a single letter A, B, C, or D
        (optionally followed by punctuation or whitespace).
        
        Args:
            text: Generated text from model
            
        Returns:
            Answer choice ('A', 'B', 'C', 'D') or empty string if not found
        """
        # Remove leading/trailing whitespace
        text = text.strip()
        
        # Look for single letter A, B, C, or D at the start
        # Pattern: letter at start, optionally followed by punctuation/whitespace
        match = re.search(r'^([ABCD])[\.\)\s,;:]*', text, re.IGNORECASE)
        if match:
            return match.group(1).upper()
        
        # Look for letter in parentheses or brackets
        match = re.search(r'[\(\[]([ABCD])[\)\]]', text, re.IGNORECASE)
        if match:
            return match.group(1).upper()
        
        # Look for "Answer: A" or "The answer is B" patterns
        patterns = [
            r'answer[:\s]+([ABCD])',
            r'the\s+answer\s+is\s+([ABCD])',
            r'correct\s+answer[:\s]+([ABCD])',
            r'choice\s+([ABCD])',
        ]
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).upper()
        
        # Look for any standalone A, B, C, or D in the first 50 characters
        short_text = text[:50]
        match = re.search(r'\b([ABCD])\b', short_text, re.IGNORECASE)
        if match:
            return match.group(1).upper()
        
        return ""

    def compute_accuracy(
        self,
        predictions: List[str],
        references: List[int],
    ) -> Tuple[float, List[bool], List[str]]:
        """Compute accuracy for multiple-choice questions.
        
        Args:
            predictions: List of predicted answer texts (generated by model)
            references: List of correct answer indices (0=A, 1=B, 2=C, 3=D)
            
        Returns:
            Tuple of (accuracy, correctness_list, extracted_answers)
        """
        assert len(predictions) == len(references), (
            f"Mismatch: {len(predictions)} predictions vs {len(references)} references"
        )
        
        correct = []
        extracted = []
        
        for pred, ref_idx in zip(predictions, references):
            # Extract answer choice from prediction
            answer_letter = self.extract_answer(pred)
            extracted.append(answer_letter)
            
            # Convert reference index to letter
            ref_letter = chr(ord('A') + ref_idx)
            
            # Check if correct
            is_correct = (answer_letter == ref_letter)
            correct.append(is_correct)
        
        accuracy = sum(correct) / len(correct) if len(correct) > 0 else 0.0
        
        return accuracy, correct, extracted

    def compute_accuracy_by_choice(
        self,
        predictions: List[str],
        references: List[int],
    ) -> dict:
        """Compute accuracy broken down by answer choice.
        
        Args:
            predictions: List of predicted answer texts
            references: List of correct answer indices
            
        Returns:
            Dictionary with accuracy per choice and overall accuracy
        """
        accuracy, correct, extracted = self.compute_accuracy(predictions, references)
        
        # Count by choice
        choice_counts = {'A': 0, 'B': 0, 'C': 0, 'D': 0}
        choice_correct = {'A': 0, 'B': 0, 'C': 0, 'D': 0}
        
        for ext, ref_idx, is_corr in zip(extracted, references, correct):
            ref_letter = chr(ord('A') + ref_idx)
            if ext in choice_counts:
                choice_counts[ext] += 1
                if is_corr:
                    choice_correct[ext] += 1
        
        # Compute accuracy per choice
        choice_accuracy = {}
        for choice in ['A', 'B', 'C', 'D']:
            if choice_counts[choice] > 0:
                choice_accuracy[choice] = choice_correct[choice] / choice_counts[choice]
            else:
                choice_accuracy[choice] = 0.0
        
        return {
            "overall_accuracy": accuracy,
            "choice_accuracy": choice_accuracy,
            "choice_counts": choice_counts,
            "correctness": correct,
            "extracted_answers": extracted,
        }
