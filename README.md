# Efficient-LLM-Inference
Sparse/pruned attention + KV-cache optimization for efficient LLM inference on small/medium models; benchmark latency, memory, throughput, and qualityâ€”especially for long-context.
